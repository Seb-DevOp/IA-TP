import torch

import librosa

import gradio as gr

from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor
 
# Charger mod√®le et processeur

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct")

model = Qwen2AudioForConditionalGeneration.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct", device_map="auto")
 
# Fonction de traitement de l'audio

def analyse_audio(file_obj):

    if file_obj is None:

        return "Erreur : Aucun fichier audio re√ßu."
 
    # Charger l'audio

    audio_array, _ = librosa.load(file_obj.name, sr=processor.feature_extractor.sampling_rate)
 
    # Pr√©parer la conversation

    conversation = [

        {"role": "user", "content": [

            {"type": "audio", "audio": audio_array},

        ]}

    ]
 
    # G√©n√©rer le texte ChatML

    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
 
    # Pr√©parer les entr√©es

    inputs = processor(text=text, audio=audio_array, return_tensors="pt", padding=True)

    inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
 
    # G√©n√©rer

    generate_ids = model.generate(**inputs, max_new_tokens=256)

    generate_ids = generate_ids[:, inputs["input_ids"].size(1):]
 
    # D√©coder

    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

    return response
 
# Interface Gradio

interface = gr.Interface(

    fn=analyse_audio,

    inputs=gr.File(label="Choisissez votre fichier audio (WAV ou MP3)"),

    outputs="text",

    title="üé§ Analyse audio avec Qwen2-Audio",

    description="T√©l√©verse un fichier audio (.wav ou .mp3), l'IA analyse ce qu'elle entend ! üöÄ"

)
 
# Lancer

interface.launch()

 